\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps� with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\usepackage{amsmath}

\title{HomeActivity: Recognizing home activities using sensor data}
\author{Stephen Lee, Patrick Pagus and Dong Chen}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}
\section{Abstract:}
Home activities recognizing allows many smart applications including healthcare and energy efficiency. 

\section{Introduction}

Activity recognition is the problem of determining an individual's or group's activities given some sensor data describing their actions. More formally, activity recognition finds a function that maps a sequence of sensor readings (feature vectors), to a sequence of activity labels that best describes the sensor data. We focus on activity recognition within home based wireless sensor networks. 

In this case, sensors may include contact, motion, temperature, and humidity sensors located on doors, appliances or monitoring certain rooms. This problem has many applications including healthcare and energy efficiency, where other sensors, such as cameras and wearables, intrude upon privacy or are too expensive or inconvenient. In healthcare, home activity recognition can be used to assess the cognitive and physical capabilities of an elderly person. 

Home energy efficiency can be improved given a schedule of the occupant's activities by scheduling background loads, such as heating, cooling, and various appliances. This domain has several challenges. First, there is a dearth of training data because labeling one's activities is a tedious time consuming process, this limits the number of effective techniques. Second, patterns in training data are specific to the home and the individual. Therefore, the performance of a machine learning technique may vary widely over different data sets. Finally, dominant classes may confound machine learning techniques.

To further our knowledge of machine learning, we will conduct a comprehensive empirical analysis of proposed methods in this domain, across a collection of datasets. These methods include Hidden Markov Models, Conditional Random Fields, and Support Vector Machines. We will apply these methods to at least four datasets that vary in duration, home size, and sensor counts, which are not necessarily proportional to home size. By applying these techniques to real-world datasets and explaining the differences and similarities in their performances, we will gain a deeper understanding of machine learning.

For our group project, we tackle the problem of activity recognition using available sensor information installed in homes. These sensors could be infrared motion detector sensors (to understand when a person entered a room or opened a fridge), to RFID sensors, to microphone based labeling of activities to record the ground truth. These datasets are usually available in the form of timestamp and the corresponding activity or sensor label. So, in this project we would like to predict a label (i.e. the activity) given a sequence of observations. 

\begin{enumerate}
 
\item Interesting because it has applications that spans across various domains:
	\begin{enumerate}
	\item Healthcare\\
		- Long term monitoring of activities could provide us interesting insights on degenerative heath \\
		- Especially useful in monitoring health of elderly people\\
	\item  Energy savings \\
		- Understanding the pattern of activities could help us save energy. \\
		- For example, switching off AC/heater when no one is at home \\
	\item  From a Security perspective\\
		- Again, notifications can be sent to alert the home owners if there is an aberrant activity in the door entrance. \\
	\item  Intelligent Homes\\
		- 
	\end{enumerate}
\item Challenging
		\begin{enumerate}
	\item Sensor datasets could be noisy\\
		- the front doors may open and close multiple times; this doesn't may not indicate that the person has left the home.\\
		- sometimes the sensors itself may have false positives. \\
		
	\item multiple labels may be mapped to a single sensor activity\\
		- opening of a fridge may mean both; make coffee and make cooking\\
		- some other information such as how long the person spent in the kitchen may provide some useful insight\\
		
	\item Imbalanced class problem\\
		- machine learning algorithms and works best when the number of instances of each classes are roughly equal\\
		- where the total number of a class of data (positive) is far less than the total number of another class of data (negative)	\\
		- In this problem, no activity is a perfectly reasonable label. So the prediction model may always output 'no activity' and be accurate 80\% of the time.\\
		\end{enumerate}

\end{enumerate}

\section{Research Problem}


\section{Recognizing Methods}
We would like to explore the following techniques:
	\begin{enumerate}
\item HMM - 
Hidden Markov Modes (HMM) are a buiqutious tool for modelling time serises  data.  HMMs can be used as black-box density models on sequences. They have the advantage over Markov models in that they can represent long-range dependencies between observations, mediated via the latent variables. 

\item CRF - 
CRF is a discriminative undirected probabilistic graphical model naturally applicable to sequence labeling tasks. A conditional random field or CRF (Lafferty et al. 2001), sometimes a discriminative random field (Kumar and Hebert 2003), is  a version of an MRF where all the clique potentials are conditioned on input features. The advantage of a CRF over an MRF is analogous to the advantage of a discriminative classifier over a generative classifier (see Section 8.6), namely, we don’t need to “waste resources” modeling things that we always observe. Instead we can focus our attention on modeling what we care about, namely the distribution of labels given the data.
Another important advantage of CRFs is that we can make the potentials (or factors) of the model be data-dependent. For example, in image processing applications, we may “turn off” the label smoothing between two neighboring nodes s and t if there is an observed discontinuity in the image intensity between pixels s and t. Similarly, in natural language processing problems, we can make the latent labels depend on global properties of the sentence. 

\item SVM - 
The implementation used in this experiment comes from SVM

\item SSVM - 

\item Naive Bayes
	\end{enumerate}
The idea of the naive Bayes classifier is to use a generative model of text to estimate


The datasets available to us are:
	\begin{enumerate}
\item Kasteren dataset - has over a month long sensor information from 3 homes.
\item Tulum dataset - more than six month period from a single home
	\end{enumerate}


\section{Experimental Evaluation}

\subsection{Datasets}

\subsection{Experimental Setup}

\subsection{Feather Representation}

 Raw

Changepoint

Last-fired


\subsection{Comparative Analysis Metrics}

\begin{equation}
Precision = \frac{TP}{TP+FP}
\end{equation}

\begin{equation}
Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

\begin{equation}
F-Measure = \frac{2\cdot Precision\cdot Recall}{Precison+Recall}
\end{equation}

The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation.

\begin{equation}
MCC=\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)\cdot (TP+FN)\cdot (TN+FP)\cdot (TN+FN)}}
\end{equation}


\subsection{Experiments}

	Activity recognition is the problem of determining an individual's or group's activities given some sensor data describing their actions.
	More formally, activity recognition finds a function that maps a sequence of sensor readings (feature vectors), to a sequence of activity labels that best describes the sensor data.
	We focus on activity recognition within home based wireless sensor networks.
	In this case, sensors may include contact, motion, temperature, and humidity sensors located on doors, appliances or monitoring certain rooms.
	This problem has many applications including healthcare and energy efficiency, where other sensors, such as cameras and wearables, intrude upon privacy or are too expensive or inconvenient.
	In healthcare, home activity recognition can be used to assess the cognitive and physical capabilities of an elderly person.
	Home energy efficiency can be improved given a schedule of the occupant's activities by scheduling background loads, such as heating, cooling, and various appliances.

	This domain has several challenges.
	First, there is a dearth of training data because labeling one's activities is a tedious time consuming process, this limits the number of effective techniques.
	Second, patterns in training data are specific to the home and the individual.
	Therefore, the performance of a machine learning technique may vary widely over different data sets.
	Finally, dominant classes may confound machine learning techniques.

	To further our knowledge of machine learning, we will conduct a comprehensive empirical analysis of proposed methods in this domain, across a collection of datasets.
	These methods include Hidden Markov Models, Conditional Random Fields, and Support Vector Machines.
	We will apply these methods to at least four datasets that vary in duration, home size, and sensor counts, which are not necessarily proportional to home size.
	By applying these techniques to real-world datasets and explaining the differences and similarities in their performances, we will gain a deeper understanding of machine learning.

\section{Discussion and Future Work}
\end{document}  